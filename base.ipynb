{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c49cd517-416a-47cf-bf15-b5d81228ec0c",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation Demo - Base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d81bc7-9aec-4b06-b31e-da9773c4c9c4",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Make a `.env` file\n",
    "\n",
    "```sh\n",
    "cp .env.example .env\n",
    "```\n",
    "\n",
    "### Provide API keys in `.env` file \n",
    "\n",
    "- LANGCHAIN_API_KEY (optional)\n",
    "  -  Sign up to https://smith.langchain.com/\n",
    "  -  Go to Settings -> API Keys\n",
    "  -  Create API Key\n",
    "- GROQ_API_KEY\n",
    "  - Sign up to https://console.groq.com/\n",
    "  - Go to API Keys\n",
    "  - Create API Key\n",
    "\n",
    "### Check that everything is working\n",
    "\n",
    "- Check that the configured Kernel is `.venv`\n",
    "  - In the navigation bar, click `Kernel`\n",
    "  - Click `Change Kernel...`\n",
    "- Check that all cells are working\n",
    "  - In the navigation bar, click `Kernel`\n",
    "  - Click `Restart Kernel...`\n",
    "  - In the navigation bar, click `Edit`\n",
    "  - Click `Clear Outputs of All Cells`\n",
    "  - In the navigation bar, click `Run`\n",
    "  - Click `Run All Cells`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c8f9cc-50c7-4838-a3a9-ec79ee0e0480",
   "metadata": {},
   "source": [
    "## Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7acb2bd-728f-4deb-89f0-6a00d4478fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46657b90-4a35-4443-a67c-a9cd3e697a13",
   "metadata": {},
   "source": [
    "## Set up tracing (Optional)\n",
    "Define `LANGCHAIN_API_KEY` in your `.env` file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "457a528f-5971-44b8-aa72-c594ca80b7bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'true'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if \"LANGCHAIN_API_KEY\" in os.environ and os.environ[\"LANGCHAIN_API_KEY\"] != \"\":\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "else:\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aab1ae7-b891-4a6d-aa33-a499a76a5c99",
   "metadata": {},
   "source": [
    "## Set up LLM\n",
    "Define `GROQ_API_KEY` in your `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c35c49d-248f-43c7-82d6-d18dfc7d9f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f30fd11-ddb6-41ee-aa15-1c540ab62b90",
   "metadata": {},
   "source": [
    "## Indexing - Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "682d9746-8b51-4ad0-899e-94a56eea824e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43131"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64c2a33-fe18-4846-86b0-0c12e66182c7",
   "metadata": {},
   "source": [
    "## Indexing - Split documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ec74fe9-3ab5-4e75-9e17-93ba3dd19eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e35c5ca-8e11-4e12-be9c-cfdbb6731999",
   "metadata": {},
   "source": [
    "## Indexing - Embed and store documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82953a89-8ade-4141-9ce5-d636e396aa33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing started...\n",
      "Indexing completed in 7.17s.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "print(\"Indexing started...\")\n",
    "start = time.time()\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=HuggingFaceEmbeddings())\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Indexing completed in {round(end - start, 2)}s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ca037b-3c19-401f-828e-f7b402875a61",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1e5dedc-1a55-45ec-a9a6-804e7d7919dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 6},\n",
    ")\n",
    "retrieved_docs = retriever.invoke(\"What are the approaches to Task Decomposition?\")\n",
    "\n",
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93cbef8-f8ef-4336-b90f-42774d53aacc",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b42ec55a-613b-4b68-85ae-eda35a43d128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: Is this an example question? \\nContext: This document is a filler and will be used for example contexts \\nAnswer:\")]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "example_messages = prompt.invoke({\n",
    "    \"context\": \"This document is a filler and will be used for example contexts\",\n",
    "    \"question\": \"Is this an example question?\",\n",
    "}).to_messages()\n",
    "\n",
    "example_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "644569f0-f7a7-4c66-af4d-42a223546b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is the process of breaking down a problem into smaller, manageable subtasks or thought steps. This can be done through simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", using task-specific instructions, or with human inputs."
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in rag_chain.stream(\"What is Task Decomposition?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
